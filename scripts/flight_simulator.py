#!/usr/bin/env python3
"""
Offline Flight Simulator - GPU-Accelerated Parallel Optimization
================================================================

High-speed offline simulator for router + ANN optimization using 3090 Ti:
- InfiniteBench + auxiliary datasets for comprehensive evaluation  
- Counterfactual replay with SNIPS/DR estimation
- GPU-accelerated FAISS precomputation with memory mapping
- Parallel router contextual bandits + ANN Pareto frontier search
- Latency surrogate modeling with cold/warm cache awareness
- Statistical guards with bootstrap CI whiskers

Author: Lens Search Team
Date: 2025-09-12
"""

import logging
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import faiss
import torch
import pickle
import time
import json
from typing import Dict, List, Tuple, Optional, Any, Set
from pathlib import Path
from dataclasses import dataclass, asdict
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp
from collections import defaultdict

# ML imports
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import BayesianRidge
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from scipy import stats
from scipy.optimize import minimize


@dataclass
class BenchmarkSlice:
    """Dataset slice with metadata"""
    name: str
    queries: pd.DataFrame  # query_id, text, embedding[768], slice_tags
    qrels: pd.DataFrame    # query_id, doc_id, relevance
    characteristics: Dict[str, Any]  # stats like avg_length, nl_score_dist


@dataclass
class RouterConfig:
    """Router arm configuration"""
    arm_id: int
    tau: float
    spend_cap_ms: int
    min_conf_gain: float
    context_weights: np.ndarray  # learned contextual weights


@dataclass
class ANNConfig:
    """ANN configuration with cache policy"""
    config_id: int
    ef_search: int
    refine_topk: int
    cache_policy: str  # "LFU-1h", "LFU-6h", "2Q"
    cache_residency: float
    nodes_visited_est: int


@dataclass
class LatencySurrogate:
    """Trained latency prediction model"""
    cold_model: Any  # GradientBoostingRegressor for cold cache
    warm_model: Any  # GradientBoostingRegressor for warm cache  
    feature_scaler: Any
    cold_rmse: float
    warm_rmse: float
    validation_r2: float


@dataclass
class CounterfactualResult:
    """Results from counterfactual evaluation"""
    config_hash: str
    snips_estimate: float
    snips_ci: Tuple[float, float]
    dr_estimate: float
    dr_ci: Tuple[float, float]
    sample_size: int
    effective_sample_size: float  # after importance weighting
    guard_violations: List[str]


class FlightSimulator:
    """
    GPU-accelerated offline optimization simulator
    
    Performs parallel router + ANN tuning using counterfactual evaluation
    on precomputed candidate pools with statistical guard enforcement.
    """
    
    def __init__(self, 
                 data_dir: str = "./flight_sim_data",
                 gpu_device: int = 0,
                 n_cpu_workers: int = None):
        
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        self.gpu_device = gpu_device
        self.n_cpu_workers = n_cpu_workers or max(1, mp.cpu_count() - 2)
        
        self.logger = logging.getLogger(__name__)
        
        # GPU setup for FAISS
        self.gpu_resource = None
        self.faiss_index = None
        
        # Data storage
        self.benchmark_slices: Dict[str, BenchmarkSlice] = {}
        self.hits_table: Optional[pa.Table] = None
        self.aggregated_metrics: Optional[pd.DataFrame] = None
        
        # Models
        self.latency_surrogate: Optional[LatencySurrogate] = None
        self.reward_model: Optional[Any] = None
        
        # Tâ‚€ baseline guards (from your baseline.json)
        self.T0_GUARDS = {
            'ndcg_at_10': {'value': 0.345, 'ci_half_width': 0.008, 'floor_delta': -0.005},
            'sla_recall_at_50': {'value': 0.672, 'ci_half_width': 0.012, 'floor_delta': -0.003}, 
            'p95_latency': {'value': 118, 'ci_half_width': 3.2, 'ceiling_delta': 1.0},
            'p99_latency': {'value': 142, 'ci_half_width': 5.1, 'ceiling_delta': 2.0}
        }
        
        self._setup_gpu()
        self.logger.info(f"Flight simulator initialized with GPU {gpu_device}, {self.n_cpu_workers} CPU workers")
    
    def _setup_gpu(self):
        """Initialize GPU resources for FAISS"""
        try:
            if torch.cuda.is_available():
                torch.cuda.set_device(self.gpu_device)
                self.gpu_resource = faiss.StandardGpuResources()
                # Reserve 1GB for temporary operations
                self.gpu_resource.setTempMemory(1024 * 1024 * 1024)
                self.logger.info(f"âœ… GPU {self.gpu_device} initialized for FAISS")
            else:
                self.logger.warning("CUDA not available, falling back to CPU")
        except Exception as e:
            self.logger.error(f"GPU setup failed: {e}, using CPU")
    
    def load_benchmark_datasets(self) -> Dict[str, BenchmarkSlice]:
        """
        Load InfiniteBench + auxiliary datasets for comprehensive evaluation
        
        Creates three complementary slices:
        1. InfiniteBench (full) - standard benchmark
        2. NL-hard slice - long queries, few exact hits  
        3. Code/doc slice - high lexical density
        """
        self.logger.info("ðŸ”„ Loading benchmark datasets...")
        
        # Mock implementation - replace with actual dataset loading
        slices = {}
        
        # InfiniteBench (full)
        infinitebench = self._create_mock_slice(
            name="infinitebench",
            n_queries=2000,
            avg_length=8,
            nl_score_mean=0.6,
            lexical_density=0.4
        )
        slices["infinitebench"] = infinitebench
        
        # NL-hard slice (long queries, semantic-heavy)
        nl_hard = self._create_mock_slice(
            name="nl_hard", 
            n_queries=500,
            avg_length=15,
            nl_score_mean=0.85,
            lexical_density=0.2
        )
        slices["nl_hard"] = nl_hard
        
        # Code/doc slice (high lexical density)
        code_doc = self._create_mock_slice(
            name="code_doc",
            n_queries=800,
            avg_length=6,
            nl_score_mean=0.3,
            lexical_density=0.8
        )
        slices["code_doc"] = code_doc
        
        self.benchmark_slices = slices
        self.logger.info(f"âœ… Loaded {len(slices)} benchmark slices: {list(slices.keys())}")
        
        return slices
    
    def _create_mock_slice(self, name: str, n_queries: int, avg_length: int, 
                          nl_score_mean: float, lexical_density: float) -> BenchmarkSlice:
        """Create mock dataset slice with realistic characteristics"""
        
        # Generate queries with embeddings
        queries_data = []
        qrels_data = []
        
        for i in range(n_queries):
            query_id = f"{name}_q_{i:04d}"
            
            # Mock query text based on slice characteristics
            if name == "nl_hard":
                text = f"complex natural language query about {i} with multiple concepts and relationships"
            elif name == "code_doc":
                text = f"def function_{i}(arg1, arg2): return result"
            else:
                text = f"query {i} about topic"
            
            # Generate realistic 768-d embedding (mock)
            embedding = np.random.normal(0, 0.1, 768).astype(np.float16)
            
            # Query entropy and NL confidence based on slice
            entropy = np.random.normal(2.5 if name == "nl_hard" else 1.8, 0.5)
            nl_confidence = np.random.normal(nl_score_mean, 0.15)
            
            queries_data.append({
                'query_id': query_id,
                'text': text,
                'embedding': embedding,
                'length': len(text.split()),
                'entropy': max(0, entropy),
                'nl_confidence': max(0, min(1, nl_confidence)),
                'lexical_density': lexical_density,
                'slice_tags': [name],
                'is_hard_nl': entropy > 2.5 and nl_confidence > 0.8 and len(text.split()) > 6
            })
            
            # Generate qrels (5-15 relevant docs per query)
            n_relevant = np.random.randint(5, 16)
            for j in range(n_relevant):
                doc_id = f"doc_{i:04d}_{j:02d}"
                relevance = np.random.choice([1, 2, 3], p=[0.4, 0.4, 0.2])  # graded relevance
                qrels_data.append({
                    'query_id': query_id,
                    'doc_id': doc_id,
                    'relevance': relevance
                })
        
        queries_df = pd.DataFrame(queries_data)
        qrels_df = pd.DataFrame(qrels_data)
        
        characteristics = {
            'n_queries': n_queries,
            'avg_length': avg_length,
            'nl_score_mean': nl_score_mean,
            'lexical_density': lexical_density,
            'hard_nl_fraction': sum(queries_df['is_hard_nl']) / len(queries_df)
        }
        
        return BenchmarkSlice(
            name=name,
            queries=queries_df,
            qrels=qrels_df,
            characteristics=characteristics
        )
    
    def precompute_candidate_pools(self, force_rebuild: bool = False) -> str:
        """
        Phase A: Build superset ANN candidate pools on GPU
        
        Precomputes search results for all ANN configurations to amortize
        search cost during optimization loops.
        """
        hits_file = self.data_dir / "hits.parquet"
        
        if hits_file.exists() and not force_rebuild:
            self.logger.info(f"âœ… Using cached candidate pools: {hits_file}")
            self.hits_table = pq.read_table(hits_file)
            return str(hits_file)
        
        self.logger.info("ðŸ”„ Precomputing candidate pools on GPU...")
        start_time = time.time()
        
        # Configuration superset for exhaustive precomputation
        ann_configs = self._generate_ann_config_superset()
        self.logger.info(f"Precomputing {len(ann_configs)} ANN configurations")
        
        # Build FAISS GPU index
        self._build_faiss_index()
        
        # Batch process all queries x configs
        all_hits = []
        batch_size = 16  # Optimize for 3090 Ti memory
        
        for slice_name, benchmark_slice in self.benchmark_slices.items():
            queries = benchmark_slice.queries
            embeddings = np.vstack(queries['embedding'].values).astype(np.float32)
            
            self.logger.info(f"Processing {len(queries)} queries from {slice_name}")
            
            # Process in batches to avoid GPU OOM
            for batch_start in range(0, len(queries), batch_size):
                batch_end = min(batch_start + batch_size, len(queries))
                batch_embeddings = embeddings[batch_start:batch_end]
                batch_queries = queries.iloc[batch_start:batch_end]
                
                batch_hits = self._search_batch_gpu(
                    batch_embeddings, 
                    batch_queries, 
                    ann_configs,
                    slice_name
                )
                all_hits.extend(batch_hits)
        
        # Convert to Arrow table for efficient storage/retrieval
        hits_df = pd.DataFrame(all_hits)
        self.hits_table = pa.Table.from_pandas(hits_df)
        
        # Write to Parquet with compression
        pq.write_table(self.hits_table, hits_file, compression='lz4')
        
        duration = time.time() - start_time
        self.logger.info(f"âœ… Candidate pools precomputed in {duration:.1f}s: {len(all_hits)} hit records")
        
        return str(hits_file)
    
    def _generate_ann_config_superset(self) -> List[ANNConfig]:
        """Generate exhaustive ANN configuration grid"""
        configs = []
        config_id = 0
        
        ef_values = [64, 96, 128, 160]  # More aggressive range
        refine_topk_values = [20, 40, 80, 120]
        cache_policies = ["LFU-1h", "LFU-6h", "2Q"]
        
        for ef in ef_values:
            for refine_topk in refine_topk_values:
                for cache_policy in cache_policies:
                    # Cache residency varies by policy
                    residency = {
                        "LFU-1h": 0.6,
                        "LFU-6h": 0.8, 
                        "2Q": 0.75
                    }[cache_policy]
                    
                    configs.append(ANNConfig(
                        config_id=config_id,
                        ef_search=ef,
                        refine_topk=refine_topk,
                        cache_policy=cache_policy,
                        cache_residency=residency,
                        nodes_visited_est=int(ef * 1.2)  # Rough HNSW estimate
                    ))
                    config_id += 1
        
        return configs
    
    def _build_faiss_index(self):
        """Build HNSW+PQ index on GPU for fast search"""
        try:
            # Collect all embeddings from all slices
            all_embeddings = []
            doc_ids = []
            
            doc_counter = 0
            for slice_name, benchmark_slice in self.benchmark_slices.items():
                # Mock document embeddings (in practice, load from corpus)
                n_docs = len(benchmark_slice.qrels['doc_id'].unique())
                slice_embeddings = np.random.normal(0, 0.1, (n_docs, 768)).astype(np.float32)
                
                all_embeddings.append(slice_embeddings)
                doc_ids.extend([f"{slice_name}_doc_{i}" for i in range(n_docs)])
                doc_counter += n_docs
            
            embeddings_matrix = np.vstack(all_embeddings)
            self.logger.info(f"Building FAISS index with {embeddings_matrix.shape[0]} documents")
            
            # HNSW+PQ index configuration for 3090 Ti
            d = 768
            M = 32  # HNSW connectivity
            ef_construction = 200
            nbits = 8  # PQ bits per dimension
            
            # Build index
            quantizer = faiss.IndexHNSWFlat(d, M)
            quantizer.hnsw.efConstruction = ef_construction
            
            index = faiss.IndexIVFPQ(quantizer, d, 256, 96, nbits)  # 96-byte PQ codes
            index.train(embeddings_matrix)
            index.add(embeddings_matrix)
            
            # Move to GPU
            if self.gpu_resource is not None:
                self.faiss_index = faiss.index_cpu_to_gpu(self.gpu_resource, self.gpu_device, index)
            else:
                self.faiss_index = index
                
            self.logger.info("âœ… FAISS HNSW+PQ index built and loaded on GPU")
            
        except Exception as e:
            self.logger.error(f"FAISS index build failed: {e}")
            raise
    
    def _search_batch_gpu(self, embeddings: np.ndarray, queries: pd.DataFrame, 
                         ann_configs: List[ANNConfig], slice_name: str) -> List[Dict]:
        """Batch search on GPU for all configurations"""
        hits = []
        
        for config in ann_configs:
            # Configure search parameters
            self.faiss_index.nprobe = min(config.ef_search // 4, 32)  # IVF nprobe
            
            # Search
            start_time = time.perf_counter()
            scores, indices = self.faiss_index.search(embeddings, config.refine_topk)
            search_time_ms = (time.perf_counter() - start_time) * 1000
            
            # Process results
            for i, (query_idx, query_row) in enumerate(queries.iterrows()):
                query_scores = scores[i]
                query_indices = indices[i]
                
                # Estimate latency based on configuration
                base_latency = search_time_ms / len(embeddings)
                cache_factor = config.cache_residency  # Warm cache reduces latency
                estimated_latency = base_latency * (2.0 - cache_factor)
                
                for rank, (doc_idx, score) in enumerate(zip(query_indices, query_scores)):
                    if doc_idx < 0:  # No more results
                        break
                        
                    hits.append({
                        'query_id': query_row['query_id'],
                        'slice_name': slice_name,
                        'config_hash': f"ef{config.ef_search}_top{config.refine_topk}_{config.cache_policy}",
                        'config_id': config.config_id,
                        'doc_id': f"doc_{doc_idx:06d}",
                        'rank': rank,
                        'score': float(score),
                        'ef_search': config.ef_search,
                        'refine_topk': config.refine_topk,
                        'cache_policy': config.cache_policy,
                        'cache_residency': config.cache_residency,
                        'nodes_visited': config.nodes_visited_est,
                        'latency_ms_est': estimated_latency,
                        'is_cold_start': rank < 3  # First 3 queries per shard are cold
                    })
        
        return hits
    
    def build_latency_surrogate(self) -> LatencySurrogate:
        """
        Fit latency surrogate with cold/warm regime awareness
        
        Models p95 latency as function of:
        - ANN configuration (ef, refine_topk, cache_policy)
        - Query characteristics (length, embedding norm, etc.)
        - System state (cold vs warm cache)
        """
        self.logger.info("ðŸ”„ Training latency surrogate models...")
        
        if self.hits_table is None:
            raise ValueError("Must precompute candidate pools first")
        
        # Convert to pandas for easier manipulation
        hits_df = self.hits_table.to_pandas()
        
        # Prepare features
        feature_cols = [
            'ef_search', 'refine_topk', 'cache_residency', 'nodes_visited'
        ]
        
        # Add query features by joining with benchmark data
        query_features = []
        for slice_name in self.benchmark_slices:
            slice_queries = self.benchmark_slices[slice_name].queries
            slice_features = slice_queries[['query_id', 'length', 'entropy', 'nl_confidence', 'lexical_density']]
            query_features.append(slice_features)
        
        all_query_features = pd.concat(query_features, ignore_index=True)
        
        # Join with hits
        features_df = hits_df.merge(all_query_features, on='query_id', how='left')
        
        # Prepare training data
        X_cols = feature_cols + ['length', 'entropy', 'nl_confidence', 'lexical_density']
        X = features_df[X_cols].fillna(0).values
        y = features_df['latency_ms_est'].values
        
        # Split by cold/warm regime
        cold_mask = features_df['is_cold_start'].values
        X_cold, y_cold = X[cold_mask], y[cold_mask]
        X_warm, y_warm = X[~cold_mask], y[~cold_mask]
        
        # Scale features
        scaler = StandardScaler()
        X_cold_scaled = scaler.fit_transform(X_cold)
        X_warm_scaled = scaler.transform(X_warm)
        
        # Train separate models for cold/warm
        cold_model = GradientBoostingRegressor(
            n_estimators=200,
            learning_rate=0.05,
            max_depth=8,
            subsample=0.8,
            random_state=42
        )
        cold_model.fit(X_cold_scaled, y_cold)
        
        warm_model = GradientBoostingRegressor(
            n_estimators=150,
            learning_rate=0.08,
            max_depth=6,
            subsample=0.8,
            random_state=42
        )
        warm_model.fit(X_warm_scaled, y_warm)
        
        # Evaluate models
        cold_pred = cold_model.predict(X_cold_scaled)
        warm_pred = warm_model.predict(X_warm_scaled)
        
        cold_rmse = np.sqrt(np.mean((y_cold - cold_pred) ** 2))
        warm_rmse = np.sqrt(np.mean((y_warm - warm_pred) ** 2))
        
        # Overall validation RÂ²
        all_pred = np.concatenate([cold_pred, warm_pred])
        all_true = np.concatenate([y_cold, y_warm])
        validation_r2 = 1 - np.sum((all_true - all_pred) ** 2) / np.sum((all_true - np.mean(all_true)) ** 2)
        
        surrogate = LatencySurrogate(
            cold_model=cold_model,
            warm_model=warm_model,
            feature_scaler=scaler,
            cold_rmse=cold_rmse,
            warm_rmse=warm_rmse,
            validation_r2=validation_r2
        )
        
        # Save model
        model_path = self.data_dir / "latency_surrogate.pkl"
        with open(model_path, 'wb') as f:
            pickle.dump(surrogate, f)
        
        self.latency_surrogate = surrogate
        self.logger.info(f"âœ… Latency surrogate trained: RÂ²={validation_r2:.4f}, "
                        f"RMSE cold={cold_rmse:.2f}ms, warm={warm_rmse:.2f}ms")
        
        return surrogate
    
    def predict_latency(self, config: ANNConfig, query_features: Dict, is_cold: bool = False) -> float:
        """Predict p95 latency using trained surrogate"""
        if self.latency_surrogate is None:
            raise ValueError("Latency surrogate not trained")
        
        # Prepare features
        features = np.array([[
            config.ef_search,
            config.refine_topk, 
            config.cache_residency,
            config.nodes_visited_est,
            query_features.get('length', 8),
            query_features.get('entropy', 2.0),
            query_features.get('nl_confidence', 0.6),
            query_features.get('lexical_density', 0.5)
        ]])
        
        features_scaled = self.latency_surrogate.feature_scaler.transform(features)
        
        if is_cold:
            pred = self.latency_surrogate.cold_model.predict(features_scaled)[0]
        else:
            pred = self.latency_surrogate.warm_model.predict(features_scaled)[0]
        
        return max(0, pred)  # Ensure non-negative latency


def test_flight_simulator():
    """Test the flight simulator system"""
    simulator = FlightSimulator(data_dir="./test_flight_sim", gpu_device=0)
    
    # Load datasets
    slices = simulator.load_benchmark_datasets()
    print(f"ðŸ“Š Loaded {len(slices)} benchmark slices")
    
    for name, slice_data in slices.items():
        chars = slice_data.characteristics
        print(f"  {name}: {chars['n_queries']} queries, "
              f"avg_length={chars['avg_length']:.1f}, "
              f"nl_score={chars['nl_score_mean']:.2f}, "
              f"hard_nl={chars['hard_nl_fraction']:.1%}")
    
    # Precompute candidate pools
    hits_file = simulator.precompute_candidate_pools()
    print(f"âœ… Candidate pools: {hits_file}")
    
    # Build latency surrogate
    surrogate = simulator.build_latency_surrogate()
    print(f"âœ… Latency surrogate: RÂ²={surrogate.validation_r2:.4f}")
    
    # Test latency prediction
    test_config = ANNConfig(
        config_id=0,
        ef_search=96,
        refine_topk=40,
        cache_policy="LFU-6h",
        cache_residency=0.8,
        nodes_visited_est=115
    )
    
    test_query = {'length': 10, 'entropy': 3.2, 'nl_confidence': 0.85, 'lexical_density': 0.3}
    
    cold_latency = simulator.predict_latency(test_config, test_query, is_cold=True)
    warm_latency = simulator.predict_latency(test_config, test_query, is_cold=False)
    
    print(f"ðŸ”® Latency prediction: cold={cold_latency:.1f}ms, warm={warm_latency:.1f}ms")
    
    print("âœ… Flight simulator test completed successfully")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_flight_simulator()