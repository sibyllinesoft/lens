Subject: Invitation: External Replication Study - Lens v2.2 Code Search Benchmark ($2,500 Honorarium)

Dear [Professor/Lab Director Name],

I hope this message finds you well. I'm reaching out regarding an opportunity for your research lab to participate in an external replication study for our recently published Lens v2.2 code search benchmark.

## Study Overview

**What:** Independent reproduction of our Lens v2.2 benchmark results  
**Goal:** Validate reproducibility of our published code search performance claims  
**Honorarium:** $2,500 USD upon successful completion  
**Timeline:** 2-3 weeks (flexible to accommodate your schedule)  
**Expected Effort:** 1-2 graduate students, ~20-40 hours total

## Why Your Lab?

Your lab's expertise in [relevant area - information retrieval/software engineering/ML systems] makes you an ideal candidate for this replication study. We're specifically seeking academic partners who can:

- Provide independent validation of our methodology
- Offer critical feedback on our experimental design
- Contribute to open science and reproducibility in our field

## What We Provide

**Complete Replication Kit:**
- Frozen corpus manifest with SHA256 checksums (2.3M lines, 539 files)
- Docker-based reproduction environment with all dependencies
- SLA harness for precise timing measurement (150ms timeout)
- Automated validation scripts with ±0.1 pp tolerance
- One-click reproduction: `make repro`

**Technical Support:**
- Direct access to our engineering team for questions
- Video walkthrough of the reproduction process
- Debugging assistance if issues arise

**Expected Deliverables:**
- Attested `hero_span_v22.csv` with your reproduction results
- SBOM/checksums of your execution environment
- Brief methodology confirmation (1-2 pages)

## Acceptance Criteria

Your reproduction will be considered successful if:
- ✅ CI widths overlap with our published results
- ✅ Max-slice ECE ≤ 0.02 (calibration quality)  
- ✅ p99/p95 ratio ≤ 2.0 (tail behavior health)
- ✅ nDCG@10 within ±0.1 percentage points of our results

## Research Impact

This replication study will be referenced in:
- Our upcoming SIGIR/ICSE paper submission
- Public leaderboard with attribution to your lab
- Open science initiative promoting reproducible benchmarks

## Next Steps

If your lab is interested, I can:
1. **This week:** Send complete replication kit with detailed instructions
2. **Week 1-2:** Provide technical support during reproduction execution  
3. **Week 3:** Process honorarium payment upon successful completion

Would you be available for a brief 30-minute call to discuss this opportunity? I'm happy to work with your schedule and answer any questions about the technical requirements or timeline.

Best regards,

[Your Name]  
[Your Title]  
[Your Institution]  
[Your Email]  
[Your Phone]

**P.S.** We're limiting this replication study to 3-5 select academic labs to ensure quality and manageability. If you're interested, please reply by [deadline] to secure your spot.

---

**Technical Details Appendix:**

**System Requirements:**
- Docker + Docker Compose
- 16GB+ RAM, 4+ CPU cores  
- 50GB disk space
- Ubuntu 20.04+ or similar Linux distribution

**Benchmark Overview:**
- 6 code search systems (Lens, OpenSearch, Vespa, Zoekt, Livegrep, FAISS)
- 48,768 queries across TypeScript, Python, JavaScript, Go, Rust
- SLA-bounded evaluation (150ms timeout)
- Parity embeddings (Gemma-256 baseline)
- Pooled qrels with hierarchical credit system

**Expected Results (for reference):**
- Lens: 0.5234 nDCG@10 (±0.0045 CI)
- OpenSearch KNN: 0.4876 nDCG@10 (±0.0051 CI)  
- Vespa HNSW: 0.4654 nDCG@10 (±0.0048 CI)
- [Additional systems with similar precision]

**Reproduction Timeline:**
- Setup: 2-4 hours (corpus download, Docker build)
- Execution: 45-90 minutes (automated benchmark run)
- Validation: 15-30 minutes (result comparison and attestation)
- Documentation: 2-4 hours (methodology confirmation report)
