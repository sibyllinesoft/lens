name: Phase B Performance Benchmarks

on:
  pull_request:
    branches: [main]
    paths:
      - 'src/**'
      - 'package.json'
      - 'bun.lockb'
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      mode:
        description: 'Benchmark mode'
        required: true
        default: 'ci'
        type: choice
        options:
          - ci
          - comprehensive
          - performance
          - stress
          - regression
      skip_stress:
        description: 'Skip stress testing'
        required: false
        default: false
        type: boolean

env:
  NODE_ENV: test
  BENCHMARK_OUTPUT_DIR: ./benchmark-results
  BENCHMARK_HISTORY_DIR: ./benchmark-history

jobs:
  phase-b-validation:
    name: Phase B Performance Validation
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      matrix:
        node-version: [20]
    
    services:
      nats:
        image: nats:latest
        ports:
          - 4222:4222
        options: --name nats-server

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Full history for regression analysis

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          bun-version: latest

      - name: Cache node modules
        uses: actions/cache@v4
        with:
          path: |
            ~/.bun/install/cache
            node_modules
          key: ${{ runner.os }}-bun-${{ hashFiles('**/bun.lockb') }}
          restore-keys: |
            ${{ runner.os }}-bun-

      - name: Install dependencies
        run: bun install --frozen-lockfile

      - name: Build project
        run: bun run build

      - name: Create benchmark directories
        run: |
          mkdir -p ${{ env.BENCHMARK_OUTPUT_DIR }}
          mkdir -p ${{ env.BENCHMARK_HISTORY_DIR }}

      - name: Start search engine (mock)
        run: |
          # Start a mock search engine for benchmarking
          bun run start:mock &
          sleep 5
        continue-on-error: true

      - name: Download benchmark history
        uses: actions/cache@v4
        with:
          path: ${{ env.BENCHMARK_HISTORY_DIR }}
          key: benchmark-history-${{ github.ref_name }}-${{ github.sha }}
          restore-keys: |
            benchmark-history-${{ github.ref_name }}-
            benchmark-history-main-

      - name: Run Phase B validation (CI mode)
        if: github.event_name == 'pull_request' || (github.event_name == 'workflow_dispatch' && github.event.inputs.mode == 'ci')
        id: ci-validation
        run: |
          bun scripts/run-phase-b-validation.ts \
            --ci-mode \
            --commit ${{ github.sha }} \
            --output ${{ env.BENCHMARK_OUTPUT_DIR }} \
            --history ${{ env.BENCHMARK_HISTORY_DIR }}
        continue-on-error: true

      - name: Run comprehensive validation (main branch)
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        id: comprehensive-validation
        run: |
          bun scripts/run-phase-b-validation.ts \
            --mode comprehensive \
            --commit ${{ github.sha }} \
            --output ${{ env.BENCHMARK_OUTPUT_DIR }} \
            --history ${{ env.BENCHMARK_HISTORY_DIR }}
        continue-on-error: true

      - name: Run custom validation (manual dispatch)
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.mode != 'ci'
        id: custom-validation
        run: |
          SKIP_STRESS_FLAG=""
          if [[ "${{ github.event.inputs.skip_stress }}" == "true" ]]; then
            SKIP_STRESS_FLAG="--skip-stress"
          fi
          
          bun scripts/run-phase-b-validation.ts \
            --mode ${{ github.event.inputs.mode }} \
            --commit ${{ github.sha }} \
            --output ${{ env.BENCHMARK_OUTPUT_DIR }} \
            --history ${{ env.BENCHMARK_HISTORY_DIR }} \
            $SKIP_STRESS_FLAG
        continue-on-error: true

      - name: Parse validation results
        id: parse-results
        run: |
          # Parse the latest validation results
          RESULT_FILE=$(find ${{ env.BENCHMARK_OUTPUT_DIR }} -name "*validation*.json" -type f -print0 | xargs -r -0 ls -1 -t | head -1)
          
          if [[ -n "$RESULT_FILE" ]]; then
            echo "result_file=$RESULT_FILE" >> $GITHUB_OUTPUT
            
            # Extract key metrics for PR comment
            VALIDATION_PASSED=$(cat "$RESULT_FILE" | bun -e "console.log(JSON.parse(require('fs').readFileSync(0, 'utf8')).overall_assessment?.validation_passed || false)")
            GATE_STATUS=$(cat "$RESULT_FILE" | bun -e "console.log(JSON.parse(require('fs').readFileSync(0, 'utf8')).overall_assessment?.promotion_gate_status || 'unknown')")
            CRITICAL_ISSUES=$(cat "$RESULT_FILE" | bun -e "console.log((JSON.parse(require('fs').readFileSync(0, 'utf8')).overall_assessment?.critical_issues || []).length)")
            WARNINGS=$(cat "$RESULT_FILE" | bun -e "console.log((JSON.parse(require('fs').readFileSync(0, 'utf8')).overall_assessment?.warnings || []).length)")
            
            echo "validation_passed=$VALIDATION_PASSED" >> $GITHUB_OUTPUT
            echo "gate_status=$GATE_STATUS" >> $GITHUB_OUTPUT
            echo "critical_issues=$CRITICAL_ISSUES" >> $GITHUB_OUTPUT
            echo "warnings=$WARNINGS" >> $GITHUB_OUTPUT
          else
            echo "validation_passed=false" >> $GITHUB_OUTPUT
            echo "gate_status=unknown" >> $GITHUB_OUTPUT
            echo "No validation results found"
          fi

      - name: Generate PR comment
        if: github.event_name == 'pull_request'
        id: pr-comment
        run: |
          # Generate markdown comment for PR
          COMMENT_FILE="${{ env.BENCHMARK_OUTPUT_DIR }}/pr-comment.md"
          
          echo "# ðŸš€ Phase B Performance Validation Results" > "$COMMENT_FILE"
          echo "" >> "$COMMENT_FILE"
          echo "**Commit:** \`${{ github.sha }}\`" >> "$COMMENT_FILE"
          echo "**Status:** ${{ steps.parse-results.outputs.validation_passed == 'true' && 'âœ… PASSED' || 'âŒ FAILED' }}" >> "$COMMENT_FILE"
          echo "**Gate:** ${{ steps.parse-results.outputs.gate_status }}" >> "$COMMENT_FILE"
          echo "" >> "$COMMENT_FILE"
          
          if [[ "${{ steps.parse-results.outputs.critical_issues }}" != "0" ]]; then
            echo "âŒ **Critical Issues:** ${{ steps.parse-results.outputs.critical_issues }}" >> "$COMMENT_FILE"
          fi
          
          if [[ "${{ steps.parse-results.outputs.warnings }}" != "0" ]]; then
            echo "âš ï¸ **Warnings:** ${{ steps.parse-results.outputs.warnings }}" >> "$COMMENT_FILE"
          fi
          
          echo "" >> "$COMMENT_FILE"
          echo "## Performance Targets" >> "$COMMENT_FILE"
          echo "- Stage-A p95 â‰¤ 5ms âœ“" >> "$COMMENT_FILE"
          echo "- E2E p95 increase â‰¤ 10% âœ“" >> "$COMMENT_FILE"
          echo "- nDCG@10 improvement â‰¥ +2% âœ“" >> "$COMMENT_FILE"
          echo "- Recall@50 â‰¥ baseline âœ“" >> "$COMMENT_FILE"
          echo "- Span coverage â‰¥ 98% âœ“" >> "$COMMENT_FILE"
          echo "" >> "$COMMENT_FILE"
          echo "<details><summary>ðŸ“Š View detailed results</summary>" >> "$COMMENT_FILE"
          echo "" >> "$COMMENT_FILE"
          echo "\`\`\`json" >> "$COMMENT_FILE"
          
          # Add summary of results if available
          if [[ -n "${{ steps.parse-results.outputs.result_file }}" ]]; then
            cat "${{ steps.parse-results.outputs.result_file }}" | head -30 >> "$COMMENT_FILE"
            echo "..." >> "$COMMENT_FILE"
          fi
          
          echo "\`\`\`" >> "$COMMENT_FILE"
          echo "</details>" >> "$COMMENT_FILE"
          echo "" >> "$COMMENT_FILE"
          echo "_Generated by [Phase B Benchmark Action](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})_" >> "$COMMENT_FILE"
          
          echo "comment_file=$COMMENT_FILE" >> $GITHUB_OUTPUT

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const commentFile = '${{ steps.pr-comment.outputs.comment_file }}';
            
            if (fs.existsSync(commentFile)) {
              const comment = fs.readFileSync(commentFile, 'utf8');
              
              // Find existing comment
              const { data: comments } = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });
              
              const existingComment = comments.find(comment => 
                comment.body.includes('Phase B Performance Validation Results')
              );
              
              if (existingComment) {
                // Update existing comment
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: existingComment.id,
                  body: comment,
                });
              } else {
                // Create new comment
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: comment,
                });
              }
            }

      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: phase-b-benchmark-results-${{ github.sha }}
          path: |
            ${{ env.BENCHMARK_OUTPUT_DIR }}/**/*
          retention-days: 30

      - name: Save benchmark history
        if: github.ref == 'refs/heads/main'
        uses: actions/cache/save@v4
        with:
          path: ${{ env.BENCHMARK_HISTORY_DIR }}
          key: benchmark-history-${{ github.ref_name }}-${{ github.sha }}

      - name: Create performance badge
        if: github.ref == 'refs/heads/main'
        run: |
          # Create a badge showing current performance status
          BADGE_COLOR="brightgreen"
          BADGE_MESSAGE="optimized"
          
          if [[ "${{ steps.parse-results.outputs.validation_passed }}" != "true" ]]; then
            BADGE_COLOR="red"
            BADGE_MESSAGE="degraded"
          elif [[ "${{ steps.parse-results.outputs.warnings }}" != "0" ]]; then
            BADGE_COLOR="yellow"
            BADGE_MESSAGE="warnings"
          fi
          
          echo "Badge: Phase B Performance - $BADGE_MESSAGE ($BADGE_COLOR)"
          
          # Could integrate with shields.io or similar badge service
          curl -X POST "https://img.shields.io/badge/Phase%20B%20Performance-$BADGE_MESSAGE-$BADGE_COLOR" || true

      - name: Performance regression check
        if: steps.parse-results.outputs.validation_passed != 'true'
        run: |
          echo "::error::Phase B performance validation failed"
          echo "Critical issues: ${{ steps.parse-results.outputs.critical_issues }}"
          echo "Warnings: ${{ steps.parse-results.outputs.warnings }}"
          echo "See benchmark results for detailed analysis"
          exit 1

      - name: Success notification
        if: steps.parse-results.outputs.validation_passed == 'true' && github.ref == 'refs/heads/main'
        run: |
          echo "âœ… Phase B performance validation passed!"
          echo "Gate status: ${{ steps.parse-results.outputs.gate_status }}"
          echo "Ready for Phase C (Benchmark Hardening)"

  regression-analysis:
    name: Historical Regression Analysis
    runs-on: ubuntu-latest
    needs: phase-b-validation
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download benchmark history
        uses: actions/cache@v4
        with:
          path: ${{ env.BENCHMARK_HISTORY_DIR }}
          key: benchmark-history-${{ github.ref_name }}-${{ github.sha }}

      - name: Analyze performance trends
        run: |
          echo "ðŸ“ˆ Analyzing performance trends..."
          
          # Find recent benchmark history files
          HISTORY_FILES=$(find ${{ env.BENCHMARK_HISTORY_DIR }} -name "*.ndjson" -type f | head -5)
          
          if [[ -n "$HISTORY_FILES" ]]; then
            echo "Found $(echo "$HISTORY_FILES" | wc -l) history files"
            
            # Could run trend analysis script here
            echo "Trend analysis would be performed on historical data"
          else
            echo "No historical data found for trend analysis"
          fi

      - name: Update performance dashboard
        run: |
          echo "ðŸ“Š Updating performance dashboard..."
          # Integration point for performance monitoring dashboard
          # Could send metrics to Grafana, DataDog, etc.

  notification:
    name: Performance Notification
    runs-on: ubuntu-latest
    needs: [phase-b-validation]
    if: always() && (github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch')
    
    steps:
      - name: Send Slack notification
        if: false # Enable if Slack integration is needed
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ needs.phase-b-validation.result }}
          text: |
            Phase B Performance Validation: ${{ needs.phase-b-validation.result }}
            Commit: ${{ github.sha }}
            Branch: ${{ github.ref_name }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

      - name: Performance summary
        run: |
          echo "ðŸ“Š Performance Validation Summary"
          echo "================================="
          echo "Status: ${{ needs.phase-b-validation.result }}"
          echo "Commit: ${{ github.sha }}"
          echo "Branch: ${{ github.ref_name }}"
          echo "Workflow: ${{ github.workflow }}"