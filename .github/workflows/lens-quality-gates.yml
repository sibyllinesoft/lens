name: Lens Quality Gates

on:
  pull_request:
    branches: [main, master]
    types: [opened, synchronize, reopened]
  schedule:
    # Nightly full test at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Test type to run'
        required: true
        default: 'smoke'
        type: choice
        options:
          - smoke
          - full
      baseline_comparison:
        description: 'Enable baseline comparison'
        required: false
        default: false
        type: boolean

env:
  NODE_VERSION: '20'
  NATS_URL: 'nats://localhost:4222'
  OUTPUT_DIR: './benchmark-results'

jobs:
  # Smoke tests for PR gates - Fast feedback
  smoke-tests:
    if: github.event_name == 'pull_request' || (github.event_name == 'workflow_dispatch' && inputs.test_type == 'smoke')
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    services:
      nats:
        image: nats:2.10-alpine
        ports:
          - 4222:4222
        options: --health-cmd="nats server check" --health-interval=10s --health-timeout=5s --health-retries=3

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 2  # Need previous commit for baseline comparison

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build project
        run: npm run build

      - name: Setup benchmark environment
        run: |
          mkdir -p ${{ env.OUTPUT_DIR }}
          # Ensure test data is available
          if [ ! -d "indexed-content" ]; then
            echo "Creating minimal test corpus..."
            mkdir -p indexed-content
            # In production, this would restore from cache or trigger indexing
          fi

      - name: Run smoke benchmark tests
        id: smoke_tests
        run: |
          node -e "
          const { TestOrchestrator } = require('./dist/src/benchmark/test-orchestrator.js');
          const { GroundTruthBuilder } = require('./dist/src/benchmark/ground-truth-builder.js');
          
          async function runSmokeTests() {
            try {
              const groundTruth = new GroundTruthBuilder();
              await groundTruth.loadGoldenDataset();
              
              const orchestrator = new TestOrchestrator(
                groundTruth, 
                '${{ env.OUTPUT_DIR }}', 
                '${{ env.NATS_URL }}'
              );
              
              const config = {
                test_type: 'smoke_pr',
                trigger: 'pr_${{ github.event_name === 'pull_request' && 'updated' || 'manual' }}',
                baseline_comparison: ${{ github.event_name == 'workflow_dispatch' && inputs.baseline_comparison || false }},
                max_duration_minutes: 10,
                pr_context: ${{ github.event_name == 'pull_request' && format('{\"pr_number\": {0}, \"branch_name\": \"{1}\", \"commit_sha\": \"{2}\", \"base_branch\": \"{3}\"}', github.event.number, github.head_ref, github.sha, github.base_ref) || 'null' }}
              };
              
              const result = await orchestrator.executeSmokeTests(config);
              
              // Set outputs for later steps
              console.log('::set-output name=passed::' + result.passed);
              console.log('::set-output name=blocking::' + result.blocking_merge);
              console.log('::set-output name=quality_score::' + result.quality_score);
              console.log('::set-output name=summary_path::' + result.artifacts.summary_json);
              
              // Exit with failure if tests don't pass
              if (!result.passed) {
                console.error('üö´ Smoke tests FAILED - merge blocked');
                process.exit(1);
              } else {
                console.log('‚úÖ Smoke tests PASSED - merge allowed');
              }
              
            } catch (error) {
              console.error('‚ùå Smoke test execution failed:', error);
              process.exit(1);
            }
          }
          
          runSmokeTests().catch(console.error);
          "
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Upload smoke test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: smoke-test-results-${{ github.run_id }}
          path: |
            ${{ env.OUTPUT_DIR }}/*.json
            ${{ env.OUTPUT_DIR }}/*.ndjson
            ${{ env.OUTPUT_DIR }}/*.pdf
          retention-days: 30

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              // Load test summary
              const summaryPath = '${{ steps.smoke_tests.outputs.summary_path }}' || '${{ env.OUTPUT_DIR }}/smoke_summary.json';
              if (!fs.existsSync(summaryPath)) {
                console.log('No summary file found, skipping PR comment');
                return;
              }
              
              const summary = JSON.parse(fs.readFileSync(summaryPath, 'utf8'));
              
              const passed = '${{ steps.smoke_tests.outputs.passed }}' === 'true';
              const qualityScore = parseFloat('${{ steps.smoke_tests.outputs.quality_score }}' || '0');
              
              const statusIcon = passed ? '‚úÖ' : '‚ùå';
              const statusText = passed ? 'PASS - Ready to merge' : 'FAIL - Merge blocked';
              
              const comment = `## ${statusIcon} Lens Quality Gates Results
              
**Status:** ${statusText}  
**Quality Score:** ${(qualityScore * 100).toFixed(1)}%  
**Test Duration:** ${summary.benchmark_summary?.total_queries || 'N/A'} queries tested
              
### Quality Metrics
${summary.quality_metrics ? Object.entries(summary.quality_metrics).map(([key, value]) => 
  `- **${key}:** ${value}`
).join('\n') : 'No metrics available'}

### Gate Results
- **Preflight Checks:** ${summary.results?.preflight?.passed ? '‚úÖ PASS' : '‚ùå FAIL'}
- **Performance Gates:** ${summary.results?.performance?.passed ? '‚úÖ PASS' : '‚ùå FAIL'}

${!passed && summary.results?.performance?.tripwires_triggered?.length > 0 ? `
### Issues Found
${summary.results.performance.tripwires_triggered.map(tripwire => 
  `- ${tripwire.severity === 'error' ? 'üö®' : '‚ö†Ô∏è'} **${tripwire.name}:** ${tripwire.actual} (threshold: ${tripwire.threshold})`
).join('\n')}
` : ''}

<details>
<summary>üìä View detailed results</summary>

\`\`\`json
${JSON.stringify(summary.benchmark_summary || {}, null, 2)}
\`\`\`

</details>

---
*Lens Quality Gates powered by Phase 5 CI hardening* üîç`;

              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
              
            } catch (error) {
              console.error('Failed to post PR comment:', error);
            }

      - name: Block merge on failure
        if: failure() || steps.smoke_tests.outputs.blocking == 'true'
        run: |
          echo "üö´ Quality gates failed - merge blocked"
          exit 1

  # Full nightly tests - Comprehensive validation
  nightly-tests:
    if: github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && inputs.test_type == 'full')
    runs-on: ubuntu-latest
    timeout-minutes: 150  # 2.5 hours
    
    services:
      nats:
        image: nats:2.10-alpine
        ports:
          - 4222:4222
        options: --health-cmd="nats server check" --health-interval=10s --health-timeout=5s --health-retries=3

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build project
        run: npm run build

      - name: Setup comprehensive test environment
        run: |
          mkdir -p ${{ env.OUTPUT_DIR }}
          # In production, this would set up full test corpus
          echo "Setting up comprehensive test environment..."

      - name: Run full nightly benchmark tests
        id: nightly_tests
        run: |
          node -e "
          const { TestOrchestrator } = require('./dist/src/benchmark/test-orchestrator.js');
          const { GroundTruthBuilder } = require('./dist/src/benchmark/ground-truth-builder.js');
          
          async function runNightlyTests() {
            try {
              const groundTruth = new GroundTruthBuilder();
              await groundTruth.loadGoldenDataset();
              
              const orchestrator = new TestOrchestrator(
                groundTruth, 
                '${{ env.OUTPUT_DIR }}', 
                '${{ env.NATS_URL }}'
              );
              
              const config = {
                test_type: 'full_nightly',
                trigger: 'scheduled_nightly',
                baseline_comparison: true,
                max_duration_minutes: 120,
                generate_dashboard_data: true
              };
              
              const result = await orchestrator.executeFullNightlyTests(config);
              
              // Set outputs
              console.log('::set-output name=passed::' + result.passed);
              console.log('::set-output name=quality_score::' + result.quality_score);
              console.log('::set-output name=stability_score::' + result.stability_score);
              console.log('::set-output name=dashboard_path::' + result.artifacts.dashboard_json);
              
              if (result.passed) {
                console.log('‚úÖ Nightly tests PASSED');
              } else {
                console.log('‚ö†Ô∏è Nightly tests DEGRADED - quality issues detected');
              }
              
            } catch (error) {
              console.error('‚ùå Nightly test execution failed:', error);
              // Don't fail the job for nightly tests - just report
            }
          }
          
          runNightlyTests().catch(console.error);
          "
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Upload nightly test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: nightly-test-results-${{ github.run_id }}
          path: |
            ${{ env.OUTPUT_DIR }}/*.json
            ${{ env.OUTPUT_DIR }}/*.ndjson
            ${{ env.OUTPUT_DIR }}/*.pdf
          retention-days: 90

      - name: Update quality dashboard
        if: steps.nightly_tests.outputs.dashboard_path
        run: |
          echo "üìä Updating quality dashboard with latest results..."
          # In production, this would push to dashboard API or S3
          echo "Dashboard data available at: ${{ steps.nightly_tests.outputs.dashboard_path }}"

      - name: Send alerts on quality degradation
        if: steps.nightly_tests.outputs.passed == 'false'
        uses: actions/github-script@v7
        with:
          script: |
            // In production, this would send Slack/email alerts
            console.log('üö® Quality degradation detected in nightly tests');
            console.log('Quality Score: ${{ steps.nightly_tests.outputs.quality_score }}');
            console.log('Stability Score: ${{ steps.nightly_tests.outputs.stability_score }}');

  # Manual test execution for debugging
  manual-test:
    if: github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    services:
      nats:
        image: nats:2.10-alpine
        ports:
          - 4222:4222

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build project
        run: npm run build

      - name: Run manual test
        run: |
          echo "üîß Running manual test: ${{ inputs.test_type }}"
          mkdir -p ${{ env.OUTPUT_DIR }}
          
          # Run the specified test type
          if [ "${{ inputs.test_type }}" == "smoke" ]; then
            echo "Running smoke tests..."
            npm run benchmark:smoke
          else
            echo "Running full tests..."
            npm run benchmark:full
          fi

      - name: Upload manual test results
        uses: actions/upload-artifact@v4
        with:
          name: manual-test-results-${{ github.run_id }}
          path: ${{ env.OUTPUT_DIR }}/**/*